如果一件事情让你感觉乏味，那么你一定还没有挖掘到他有趣的一面。  
新世纪计算机技术的变革，热点从移动互联网，大数据到人工智能和区块链，越来越多的技术都被打上了去中心化和分布式的标签。回到技术的本源，掌握技术的本质，才能从更深层次理解技术。  
**一：CAP分布式系统经典理论：**  
CAP理论告诉我们，一个分布式系统不可能同时满足一致性（C：Consistency），可用性（A：Availability）和分区容错性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中的两项。
一致性：指多个副本之间是否能够保持一致的特性。在分布式系统中，如果能够做到针对一个数据项更新操作执行成功后，所有的用户都可以读取到其最新的值，那么这样的系统就被认为具有强一致性。
可用性：指系统提供的服务必须一直处于可用的状态，对于用户的每一个请求总是能够在有限的时间内返回结果。
分区容错性：分布式系统在遇到任何网络分区故障的时候，任然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。
小结：对于一个分布式系统而言，分区容错性可以说是一个最基本的要求，以及一个必然需要面对的问题。否则也就不是分布式系统了。因此系统架构设计，往往需要把精力放在如何根据业务特点在C和A之间寻求平衡。  

目前的开源分布式系统，因为其所面对的业务和系统自身设计的不同，所以其在CAP理论的侧重上也各有不同。针对这个问题我们从Hbase本身的CAP特性来进行分析，以下这段原文总结的很精辟：
The short summary of the article is that CAP isn’t “C, A, or P, choose two,” but rather “When P happens, choose A or C.”
当分区容错性发生时，我们是选择可用性还是一致性。

`Partitions, like death and taxes, are unavoidable – think of machine 
death as just a partition of that machine out into the networking 
equivalent of the afterlife. So it’s up to the system designer to 
decide if, when that happens, we give up availability or give up 
consistency.
`  

In Hbase’s case we choose consistency, so we have to give up some availability.   
Hbase在Partition tolerance下，选择了一致性，并牺牲了一些可用性。

**二：拜占庭将军问题：**  
故事：1982年，Lamport与另两人共同发表了论文The Byzantine Generals Problem，提出了一种计算机容错理论。为了将所描述的问题形象的表达出来，Lamport设想出了下面这样一个场景：
“拜占庭帝国有很多军队，不同军队的将军之间必须制定一个统一的行动计划，从而做出进攻或者撤退的决定，同时，各个将军在地理上都是被分隔开来的，只能依靠军队的通讯员来进行通讯。然而，在所有的通讯员中可能会存在叛徒，这些叛徒可以任意篡改消息，从而达到欺骗将军的目的。”
这就是著名的拜占庭将军问题。
理论上：分布式计算领域，在异步系统和不可靠通道上来达到一致性状态是不可能的，因此在对一致性的研究过程中，都往往假设信道是可靠的。
事实上：大多数系统都部署在同一个局域网中，消息被篡改的情况非常罕见。硬件和网络原因造成的消息不完整，可以通过简单的校验算法来避免。实际工程中，可以假设所有消息都是完整的，没有被篡改。
那么在这种情况下需要什么算法来保证一致性？=》 Paxos  

三：Paxos  
Lamport果然是一位善于讲故事且富有幽默感的分布式计算领域大牛，有时候故事要比数学公式更具有感染力。
故事：古希腊有一个叫做Paxos的小岛，岛上采用议会的形式来通过法令，议会中的议员通过信使进行消息的传递。值得注意的是，议员和信使都是兼职的，他们随时都有可能会离开议会厅，并且信使可能会重复的传递消息，也可能已去不复返。因此，议会协议要保证在这种情况下法令任然能够正确的产生，并且不会出现冲突。
可以看到，Lamport的所有故事里，信使都不是最可靠的，这模拟了真实生产环境里，计算机系统，经常容易出现的网络故障等现象。
Paxos是一种为了提高分布式系统容错性的一致性算法（CAP中的Consistency），算法的核心是“synod”算法。
它的目标是，要保证最终有一个提案会被选定，当提案被选定后，进程最终也能获取到被选定的提案。(Liveness)  
1：问题描述：  
假设有一组可以提出提案的程序，对于一个一致性算法来说需要保证以下几点：
（1）所有被提出的提案中，只有一个会被选定；
（2）如果没有提案被提出，那么不会有被选定的提案；
（3）当一个提案被选中后，进程应该可以获取被选定的提案信息。
对于一致性来说，安全性（Safety）需求如下：
（1）只有被提出的提案才能被选定；
（2）只能有一个值被选定；
（3）如果某个进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个。
注：一个分布式算法有两个最重要的属性：安全性（Safety）和活性（Liveness）。Safety是指那些需要保证永远都不会发生的事情，Liveness则是指那些最终一定会发生的事情。
在Paxos中，有三种参与角色，我们用Proposer（提案人），Acceptor（接受者）和Learner（学习者）来表示。
假设不同的参与者之间可以通过收发消息来进行通信，则会出现以下问题：
（1）每个参与者以任意的速度执行，可能会因为出错而停止，也可能会重启。提案选定后，所有的参与者也都有可能失败或重启，因此除非那些失败或重启的参与者可以记录某些信息，否则将无法确定最终的值。
（2）消息在传输的过程中，可能会出现不可预知的延迟，也可能会重复或丢失，但是消息不会被损坏，即消息内容不会被篡改（拜占庭问题）。

2：提案的选定：  
确定一个提案的两种方式：
一种是：只允许一个Acceptor存在，Proposer只能发提案给该Acceptor。这样就只能将它所接受的第一个提案作为被选定的提案。但是一旦这个Acceptor出现问题，那么整个系统就无法工作了。
第二种是：使用多个Acceptor来避免Acceptor的单点问题。Proposer向Acceptor集合发送提案，同样，集合中的每一个Acceptor都可能会批准该提案，当有足够多的Acceptor批准这个提案的时候，我们就可以认为该提案被选定了。（我们规定，每一个Acceptor最多只能批准一个提案）

3：推导过程（略）：  
假设在没有失败和消息丢失的情况下，我们希望即使在只有一个提案被提出的情况下，仍然可以选出一个提案，这就暗示了如下的需求。
P1：一个Acceptor必须批准它收到的第一个提案。
=》 plus 一个Acceptor必须批准不止一个提案。
我们用[编号，Value]来标识一个提案。
P2：如果编号为M0 , Value值为V0的提案（M0 , V0）被选定了，那么所有比编号M0更高的，且被Acceptor批准的提案，其Value值必须也是V0。

4：算法陈述：  
综合推导，可以对Paxos算法的提案选定过程进行一个陈述。
阶段一：
1：Proposer选择一个提案编号Mn，然后向Acceptor的某个超过半数的子集成员发送编号为Mn的Prepare请求。
2：如果一个Acceptor收到一个编号为Mn的Prepare请求，且编号Mn大于该
Acceptor已经响应的所有Prepare请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给Proposer，同时该Acceptor会承诺不会再批准任何编号小于Mn的提案。
阶段二：
1：如果Proposer收到来自半数以上的Acceptor对于其发出的编号为Mn的Prepare请求的响应，那么它就会发送一个针对[Mn , Vn]提案的Accept请求给Acceptor。注意，Vn的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它就是任意值。
2：如果Acceptor收到这个针对[Mn , Vn]提案的Accept请求，只要该Acceptor尚未对编号大于Mn的Prepare请求做出响应，它就可以通过这个提案。

总结：  
Paxos算法引入了“过半”的理念，通俗的讲就是少数服从多数的原则，同事，Paxos算法支持分布式节点角色之间的轮换，这极大的避免了分布式单点的出现，因此Paxos算法既解决了无限期等待问题，也解决了“脑裂”问题，是目前来说最优秀的分布式一致性协议之一。  

四：Paxos的工程应用  
在Google中Paxos已经广泛被应用于各种分布式开源项目中。如，Chubby，Hypertable等等。
Chubby是一个分布式锁服务，GFS和BigTable都用它来解决分布式协作，元数据存储和Master节点选举等等一系列和分布式锁服务相关的问题。
Hypertable是C++开发的开源，高性能，可伸缩的数据库，以Google BigTable相关论文为基础知道，采用与Hbase非常相似的分布式模型。目的是构建一个针对分布式海量数据的高并发数据库。
而在目前工业界应用最广的基础设施Zookeeper也是一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布/订阅，负载均衡，命名服务，分布式协调/通知，集群管理，Master选举，分布式锁和分布式队列等功能。Zookeeper并没有完全采用Paxos算法，而是使用了一种称为Zookeeper Atomic Broadcast（ZAB，ZooKeeper原子消息广播协议）的协议作为其数据一致性的核心算法，后面我们将会介绍ZAB。
故事：关于“Zookeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例：Pig），雅虎的工程师希望给这个项目也取一个动物的名字、时任研究员的首席科学家 Raghu Ramakrishnan开玩笑地说：“再这样下去，我们这儿就变成动物园了！”此话一出，大家纷纷表示就叫动物园管理员吧——因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而Zookeeper正好要用来进行分布式环境的协调——于是，Zookeeper的名字也就由此诞生了。
它可以保证如下分布式一致性特性。
顺序一致性：客户端的更新顺序与它们被发送的顺序相一致。
原子性：更新操作要么成功要么失败，没有第三种结果。
单一视图：无论客户端连接到哪一个服务器，客户端将看到相同的Zookeeper视图。
可靠性：一旦一个更新操作被应用，那么在客户端再次更新它之前，它的值不再改变。
实时性：在特定的一段时间内，客户端看到的系统需要被保证是实时的。

Zookeeper的ZAB协议核心：  
定义了对于那些会改变Zookeeper服务器数据状态的事物请求的处理方式：
所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为Leader，而余下的其他服务器是Follower。Leader负责将一个客户端事务请求转换成一个事物Proposal（提议），并将该Proposal分发给集群中所有的Follower服务器。之后Leader等待所有Follower反馈，一旦收到超过半数的Follower反馈后，那么Leader就会再次向所偶的Follower分发Commit消息，要求其将前一个Proposal进行提交。

Zookeeper的选举：  
小序号优先
ZooKeeper 需要在所有的服务器中选举出一个 Leader ，然后让这个 Leader 来负责管理集群。此时，集群中的其它服务器则成为此 Leader 的 Follower 。当 Leader 故障的时候，需要 ZooKeeper 能够快速地在 Follower 中选举出下一个 Leader 。这就是 ZooKeeper 的  Leader Election  机制。
此操作实现的核心思想是：首先创建一个 EPHEMERAL（临时） 目录节点，例如“ /election ”。然后。每一个 ZooKeeper 服务器在此目录下创建一个 SEQUENCE| EPHEMERAL 类型的节点，例如“ /election/n_ ”。在 SEQUENCE（顺序） 标志下， ZooKeeper 将自动地为每一个 ZooKeeper 服务器分配一个比前一个分配的序号要大的序号。此时创建节点的 ZooKeeper 服务器中拥有最小序号编号的服务器将成为 Leader 。
这里有一篇对Zookeeper选举写的非常生动的文章可以参考
http://blog.csdn.net/yinwenjie/article/details/47613309
实际上FastLeaderELection说的中心思想无外乎以下几个关键点：
（1）全天下我最牛，在我没有发现比我牛的推荐人的情况下，我就一直推举我当leader。第一次投票那必须推举我自己当leader。
（2）每当我接收到其它的被推举者，我都要回馈一个信息，表明我还是不是推举我自己。如果被推举者没我大，我就一直推举我当leader，是我是我还是我！
（3）我有一个票箱， 和我属于同一轮的投票情况都在这个票箱里面。一人一票 重复的或者过期的票，我都不接受。
（4）一旦我不再推举我自己了（这时我发现别人推举的人比我推荐的更牛），我就把我的票箱清空，重新发起一轮投票（这时我的票箱一定有两票了，都是选的我认为最牛的人）。
（5）一旦我发现收到的推举信息中投票轮要高于我的投票轮，我也要清空我的票箱。并且还是投当初我觉得最牛的那个人（除非当前的人比我最初的推荐牛，我就顺带更新我的推荐）。
（6）不断的重复上面的过程，不断的告诉别人“我的投票是第几轮”、“我推举的人是谁”。直到我的票箱中“我推举的最牛的人”收到了不少于 N /2 + 1的推举投票。
（7）这时我就可以决定我是flower还是leader了（如果至始至终都是我最牛，那我就是leader咯，其它情况就是follower咯）。并且不论随后收到谁的投票，都向它直接反馈“我的结果”。


五：Storm里的一致性  
ACK机制：  
Storm为了保证每条数据成功被处理,实现至少一次语义，通过Storm的ACK机制可以对spout产生的每一个tuple进行跟踪。
Storm的ACK机制：http://www.cnblogs.com/intsmaze/p/5918087.html 很详细 http://blog.csdn.net/derekjiang/article/details/9047785 更详细
Storm中有个特殊的task名叫acker，它是Ibolt的一个实现，他们负责跟踪spout发出的每一个Tuple的Tuple树（因为一个tuple通过spout发出了，经过每一个bolt处理后，会生成一个新的tuple发送出去）。当acker（框架自启动的task）发现一个Tuple树已经处理完成了，它会发送一个消息给产生这个Tuple的那个task。对任意大的一个Tuple树，storm只需要恒定的20字节就可以进行跟踪。acker对于每个spout-tuple保存一个ack-val的校验值，它的初始值是0，然后每发射一个Tuple或Ack一个Tuple时，这个Tuple的id就要跟这个校验值异或一下，
并且把得到的值更新为ack-val的新值。那么假设每个发射出去的Tuple都被ack了，那么最后ack-val的值就一定是0。Acker就根据ack-val是否为0来判断是否完全处理，如果为0则认为已完全处理。
Storm:  
1. at-most-once：spout针对相同的tuple只发送一次即可，不需要实现fail和ack方法。
2. at-least-once：是用acker机制实现的，我们需要实现spout的两个方法：fail和ack，在topology上增加一个ackerbolt，spout和bolt发出的每一个tuple都会被将跟踪信息写到ackerbolt，如果这个tuple正常被处理，则调用spout的ack方法，否则调用fail方法。
3. exactly-once：使用storm的高级部分trident实现。batch作为一个transaction的单位，一个batch包含多个tuple，transaction分成两个部分：processing和commit，processing阶段并行执行，commit阶段严格按序提交transaction状态数据到zk，在transaction的任一阶段出现问题，都会将该事务的结果扔掉，spout重发该事务的batch数据。

六：Spark Streaming的数据处理方式  
接下来是流式计算的三个概念，很显然目前我们需要用最后一种来保证数据零丢失：
At most once - 每条数据最多被处理一次（0次或1次）允许丢失
At least once  - 每条数据最少被处理一次 (1次或更多)  可能重复消费
Exactly once   - 每条数据只会被处理一次（没有数据会丢失，并且没有数据会被多次处理）
当采用思路一解决这一问题的时候，需要对receiver和direct做出选择。
当数据需要保存到Hive中，并保证数据的完整性时，我们的基本原则是，对于每一条数据，进行消费一次且仅一次的事务机制：





